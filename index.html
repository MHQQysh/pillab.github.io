<!DOCTYPE html>
<html lang="en">
    <head>
        <style>
            body {
                visibility: hidden;
            }
        </style>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="We at the Zhejiang University Probabilistic Inference and Learning (PIL) Lab tackle fundamental open problems in computer vision research and are intrigued by visual functionalities that give rise to semantically meaningful interpretations of the visual world.">
        <title>PIL lab</title>
    </head>
    <script src="assets/js/index.js" defer="defer"></script>
    <link href="assets/css/home.6d297e3c.css" rel="stylesheet">


    <body>
        <nav class="navbar" role="navigation" aria-label="main navigation">
            <div class="container">
                <div class="navbar-brand">
                    <a class="navbar-item" href="/">
                        <img height="80px" src="assets/images/logosvg.01ab8194.png">
                    </a>
                    <a class="navbar-burger" role="button" data-target="svl-navbar-menu" aria-label="menu" aria-expanded="false">
                        <span aria-hidden="true"></span>
                        <span aria-hidden="true"></span>
                        <span aria-hidden="true"></span>
                    </a>
                </div>
                <div class="navbar-menu" id="svl-navbar-menu">
                    <div class="navbar-end">
                        <a class="navbar-item active" href="/">
                            <div class="svl-navbar-item-container">Home</div>
                        </a>
                        <a class="navbar-item" href="/people">
                            <div class="svl-navbar-item-container">People</div>
                        </a>
                        <a class="navbar-item" href="/research">
                            <div class="svl-navbar-item-container">Research</div>
                        </a>
                        <a class="navbar-item" href="/publications">
                            <div class="svl-navbar-item-container">Publications</div>
                        </a>
                        <!---->
                    </div>
                </div>
            </div>
        </nav>



    <!-- ======================= -->
            <!-- 第一个块 -->
    <!-- ======================= -->
    
    <section class="section svl-home--hero">
        <div class="container">
            <div class="columns is-gapless is-desktop">
                <div class="column">
                    <div class="svl-home--hero_left svl-home--section_top" style="margin-right:40pt; text-align: left;">
                        <p class="title is-4" style="font-weight: bold;">Welcome to the Probabilistic Inference and Learning Lab (PIL)</p>
                        <p style="line-height: 1.6; text-align: justify;">We explore the foundational principles and cutting-edge algorithms in machine learning, particularly focusing on probabilistic modeling, inference, and learning. Our research bridges the gap between machine learning, information theory, and statistical mechanics, investigating common underlying principles that span across these fields. We delve into graphical models, Bayesian inference, and advanced learning algorithms to unlock deeper insights into data-driven decision-making. Our goal is to develop robust, scalable methods for reasoning under uncertainty and creating intelligent systems that can learn and adapt from complex data patterns.</p>
                        <p><span style="font-weight:600;">Join us: </span>If you are interested in research opportunities at PIL, please send an email to <a href="mailto:xiangmingmeng@intl.zju.edu.cn">Xiangming Meng</a>.</p>
                    </div>
                </div>
                <div class="column is-two-fifths">
                    <figure class="image">
                        <img src="assets/images/svl-group-2020.dc7f893c.jpg" alt="SVL Group Image 2020">
                    </figure>
                </div>
            </div>
        </div>
    </section>
    
    <style>
        .svl-home--hero_left {
            max-width: 900px; /* Adjust width for better text alignment */
            margin-left: auto;
            margin-right: auto;
        }
    
        p {
            font-size: 1rem; /* Adjust font size */
            line-height: 1.8; /* Adjust line height for better readability */
            margin-bottom: 20px; /* Add some space between paragraphs */
        }
    
        .title.is-4 {
            font-size: 2rem; /* Larger font for the title */
            margin-bottom: 10px; /* Space below the title */
        }
    </style>
    
    
    <!-- ======================= -->
        <!-- 第二个块 -->
    <!-- ======================= -->

        <section class="section">
            <div class="container">
                <div class="glide">
                    <div class="glide__track" data-glide-el="track">
                        <ul class="glide__slides">
                            <li class="glide__slide">
                                <div class="columns">
                                    <div class="column is-two-fifths is-paddingless">
                                        <a href="http://behavior.stanford.edu/">
                                            <figure class="image">
                                                <img src="assets/images/carousel-behavior.326eb7e4.jpg" alt="BEHAVIOR">
                                            </figure>
                                        </a>
                                    </div>
                                    <div class="column">
                                        <p class="title is-5">BEHAVIOR</p>
                                        <p>BEHAVIOR is a human-centered simulation benchmark to evaluate embodied AI solutions. Embodied artificial intelligence (EAI) is advancing. But where are we now? We propose to test EAI agents with the physical challenges humans need to solve in their everyday life: household activities such as doing laundry, picking up toys, setting the table, or cleaning floors. BEHAVIOR is a benchmark in simulation where EAI agents need to plan and execute navigation and manipulation strategies based on sensor information to fulfill up to 1,000 household activities. BEHAVIOR tests the ability of agents to perceive the environment, plan, and execute complex long-horizon activities that involve multiple objects, rooms, and state changes, all with the reproducibility, safety, and observability offered by a realistic physics simulation.</p>
                                        <a href="http://behavior.stanford.edu/">Link &nbsp;</a>
                                    </div>
                                </div>
                            </li>
                            <li class="glide__slide">
                                <div class="columns">
                                    <div class="column is-two-fifths is-paddingless">
                                        <a href="http://objectfolder.stanford.edu/">
                                            <figure class="image">
                                                <img src="assets/images/carousel-objectfolder.fa27bb31.jpg" alt="ObjectFolder">
                                            </figure>
                                        </a>
                                    </div>
                                    <div class="column">
                                        <p class="title is-5">ObjectFolder</p>
                                        <p>ObjectFolder models the multisensory behaviors of real objects with 1) ObjectFolder 2.0, a dataset of 1,000 neural objects in the form of implicit neural representations with simulated multisensory data, and 2) ObjectFolder Real, a dataset that contains the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. It also contains a standard benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation with sight, sound, and touch. We open source both datasets and the benchmark suite to catalyze and enable new research in multisensory object-centric learning in computer vision, robotics, and beyond.</p>
                                        <a href="http://objectfolder.org/">Link &nbsp;</a>
                                    </div>
                                </div>
                            </li>
                            <li class="glide__slide">
                                <div class="columns">
                                    <div class="column is-two-fifths is-paddingless">
                                        <a href="http://moma.stanford.edu/">
                                            <figure class="image">
                                                <img src="assets/images/carousel-moma.8be4aa02.jpg" alt="MOMA">
                                            </figure>
                                        </a>
                                    </div>
                                    <div class="column">
                                        <p class="title is-5">Multi-Object Multi-Actor (MOMA)</p>
                                        <p>Multi-Object Multi-Actor (MOMA) is a compositional and hierarchical activity recognition framework for complex activities that involve multiple humans utilizing a variety of objects to accomplish certain tasks. We introduce activity graphs as the overarching and human interpretable representation of human activities in videos and activity parsing as the task of generating activity graphs.</p>
                                        <a href="http://moma.stanford.edu/">Link &nbsp;</a>
                                    </div>
                                </div>
                            </li>
                            <li class="glide__slide">
                                <div class="columns">
                                    <div class="column is-two-fifths is-paddingless">
                                        <a href="http://pair.stanford.edu/">
                                            <figure class="image">
                                                <img src="assets/images/carousel-pair.c7cc2fb6.jpg" alt="PAIR">
                                            </figure>
                                        </a>
                                    </div>
                                    <div class="column">
                                        <p class="title is-5">People, AI & Robots Group (PAIR) </p>
                                        <p>The People, AI & Robots Group (PAIR) is a research group under the Stanford 
Vision & Learning Lab that focuses on developing methods and mechanisms 
for generalizable robot perception and control.
We work on challenging open problems at the intersection of computer vision, 
machine learning, and robotics. We develop algorithms and systems that unify in 
reinforcement learning, control theoretic modeling, and 2D/3D visual scene 
understanding to teach robots to perceive and to interact with the physical world.</p>
                                        <a href="http://pair.stanford.edu/">Link &nbsp;</a>
                                    </div>
                                </div>
                            </li>
                            <li class="glide__slide">
                                <div class="columns">
                                    <div class="column is-two-fifths is-paddingless">
                                        <a href="https://doi.org/10.1109/CVPR.2015.7298698">
                                            <figure class="image">
                                                <img src="assets/images/carousel-pac.17ec8f4b.jpg" alt="AI Assisted Care">
                                            </figure>
                                        </a>
                                    </div>
                                    <div class="column">
                                        <p class="title is-5">Partnership in AI-Assisted Care</p>
                                        <p>The Partnership in AI-Assisted Care (PAC) is an interdisciplinary collaboration
between the School of Medicine and the Computer Science department focusing on
cutting edge computer vision and machine learning technologies to solve some of
healthcare's most important problems.</p>
                                        <a href="https://aicare.stanford.edu">Link &nbsp;</a>
                                        <a href="http://www.niebles.net/images/anet_cvpr15b.jpg">Media</a>
                                    </div>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <div class="glide__bullets" data-glide-el="controls[nav]">
                        <button class="glide__bullet" data-glide-dir="=0"></button>
                        <button class="glide__bullet" data-glide-dir="=1"></button>
                        <button class="glide__bullet" data-glide-dir="=2"></button>
                        <button class="glide__bullet" data-glide-dir="=3"></button>
                        <button class="glide__bullet" data-glide-dir="=4"></button>
                    </div>
                </div>
            </div>
        </section>

    <!-- ======================= -->
        <!-- 第三个块 -->
    <!-- ======================= -->
        <section class="section">
            <div class="container">
                <div class="svl-home--section_top">
                    <h4 class="title is-4">Research at SVL</h4>
                    <p>Our research addresses the theoretical foundations and practical applications of
computational vision. We are focused on discovering and proposing the fundamental principles, algorithms and implementations for solving high-level visual perception and
cognition problems involving computational geometry, automated image and video analysis,
and visual reasoning. At the same time, our curiosity leads us to study the underlying
neural mechanisms that enable the human visual system to perform high level visual tasks
with amazing speed and efficiency.</p>
                </div>
                <div class="tile is-ancestor">
                    <div class="tile is-parent">
                        <div class="tile is-child box">
                            <figure class="image is-4by3">
                                <img src="assets/images/roberto.40017149.jpg" alt="Portfolio Image">
                            </figure>
                        </div>
                    </div>
                    <div class="tile is-parent">
                        <div class="tile is-child box">
                            <figure class="image is-4by3">
                                <img src="assets/images/jrwithsundar.b0bac43e.jpg" alt="Portfolio Image">
                            </figure>
                        </div>
                    </div>
                    <div class="tile is-parent">
                        <div class="tile is-child box">
                            <figure class="image is-4by3">
                                <img src="assets/images/pair_roboturk.f66b9e56.jpg" alt="Portfolio Image">
                            </figure>
                        </div>
                    </div>
                </div>
            </div>
        </section>



    <!-- ======================= -->
        <!-- 第四个块 -->
    <!-- ======================= -->











        <!-- <section class="section">
            <div class="container">
                <p class="title is-4">Press Coverage</p>
                <div class="columns">
                    <div class="column">
                        <a href="https://www.cnbc.com/fei-fei-li-the-biggest-perils-and-opportunities-in-ai/">
                            <figure class="image is-8by3 svl-home--client-logo">
                                <img src="assets/images/cnbc.ffb587ea.png">
                            </figure>
                        </a>
                    </div>
                    <div class="column">
                        <a href="https://www.nytimes.com/2018/10/21/business/what-comes-after-the-roomba.html">
                            <figure class="image is-8by3 svl-home--client-logo">
                                <img src="assets/images/nytimes.bd944740.png">
                            </figure>
                        </a>
                    </div>
                    <div class="column">
                        <a href="https://www.wired.com/story/fei-fei-li-artificial-intelligence-humanity/">
                            <figure class="image is-8by3 svl-home--client-logo">
                                <img src="assets/images/wired.f1e25fe9.png">
                            </figure>
                        </a>
                    </div>
                    <div class="column">
                        <a href="https://techcrunch.com/2018/09/19/jackrabbot-2-takes-to-the-sidewalks-to-learn-how-humans-navigate-politely/">
                            <figure class="image is-8by3 svl-home--client-logo">
                                <img src="assets/images/techcrunch.8f70edb0.png">
                            </figure>
                        </a>
                    </div>
                    <div class="column">
                        <a href="http://sanfrancisco.cbslocal.com/2017/01/31/socially-aware-robot-studies-behavior-of-stanford-students/">
                            <figure class="image is-8by3 svl-home--client-logo">
                                <img src="assets/images/cbs.a1649bfe.png">
                            </figure>
                        </a>
                    </div>
                    <div class="column">
                        <a href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world">
                            <figure class="image is-8by3 svl-home--client-logo">
                                <img src="assets/images/quartz.5f882ac6.png">
                            </figure>
                        </a>
                    </div>
                    <div class="column">
                        <a href="https://www.hostingadvice.com/blog/a-look-at-the-stanford-vision-and-learning-lab/">
                            <figure class="image is-8by3 svl-home--client-logo">
                                <img src="assets/images/ha.e3370b08.png">
                            </figure>
                        </a>
                    </div>
                </div>
            </div>
        </section> -->


    <!-- ======================= -->
        <!-- 第五个块 -->
    <!-- ======================= -->


 <style>
    .event {
        margin-bottom: 20px;
    }
    .event-date {
        font-weight: bold;
        margin-right: 10px;
    }
    .event-description {
        display: inline-block;
    }
</style>
<section class="section">
    <div class="container">
        <div class="svl-home--section_top">
            <h4 class="title is-4">News and Events</h4>
        </div>

        <!-- News Entries -->
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng was invited to serve as an Area Chair for <a href="https://neurips.cc//">NeurIPS 2025</a>.</div>
        </div>
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng's paper was accepted by ICLR 2025.</div>
        </div>
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng was invited to serve as an Area Chair for <a href="https://icml.cc//">ICML 2025</a>.</div>
        </div>
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng's DMPS paper has won <span style="color: red;">ACML Best Paper Runner-UP award</span>.</div>
        </div>
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng was invited to serve as an Area Chair for <a href="https://iclr.cc/">ICLR 2025</a>.</div>
        </div>
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng's paper was accepted by IJCAI 2024.</div>
        </div>
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng was invited to serve as an Area Chair for <a href="https://neurips.cc/">NeurIPS 2024</a>.</div>
        </div>
        <div class="event">
            <div class="event-date"></div> <!-- Empty date placeholder -->
            <div class="event-description">Xiangming Meng's paper <a href="https://arxiv.org/abs/2302.00919">QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models</a> was accepted by AAAI 2024.</div>
        </div>
    </div>
</section>

            
        <footer class="footer">
            <div class="container">
                <p class="title is-4">Contact Us</p>
                <div class="columns is-desktop">
                    <div class="column">
                        <p>Stanford Vision and Learning Lab (SVL)</p>
                        <p>Program Manager: Helen Roman</p>
                        <p>Email: hmroman at stanford dot edu</p>
                    </div>
                    <div class="column has-text-right-on-desktop">
                        <p>Computer Science Department</p>
                        <p>Stanford University</p>
                        <p>353 Jane Stanford Way, Stanford, CA 94305-9025</p>
                    </div>
                </div>
            </div>
        </footer>
    </body>
</html>
